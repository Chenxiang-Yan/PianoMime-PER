{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from robopianist import music\n",
    "import cv2\n",
    "import numpy as np\n",
    "from robopianist.suite.tasks import piano_with_one_shadow_hand\n",
    "from mujoco_utils import composer_utils\n",
    "from robopianist.models.hands import HandSide\n",
    "from robopianist.models.piano import piano_constants as consts\n",
    "from robopianist.music import midi_file\n",
    "import mediapipe as mp\n",
    "from utils import process_landmarks, create_detector, preprocess_frame, adjust_hand_action, val_hand_action, \\\n",
    "                    draw_landmarks_on_image, extract_finger_based_on_key, HandAction\n",
    "import os\n",
    "import pickle\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "from robopianist.suite.tasks import piano_with_shadow_hands_res\n",
    "from dm_env_wrappers import CanonicalSpecWrapper\n",
    "from robopianist.wrappers import PianoSoundVideoWrapper\n",
    "from robopianist.wrappers.deep_mimic import DeepMimicWrapper\n",
    "from robopianist.wrappers.residual import ResidualWrapper\n",
    "from robopianist.wrappers.dm2gym import Dm2GymWrapper\n",
    "from dm_env_wrappers import SinglePrecisionWrapper\n",
    "from dm_env_wrappers import DmControlWrapper\n",
    "from robopianist.wrappers.evaluation import MidiEvaluationWrapper\n",
    "from dm_control.mujoco.wrapper import mjbindings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Estimate the Homography Matrix\n",
    "\n",
    "To transform video coordinates to real world coordinates, you'll need to estimate a **homography matrix** using known landmark points.\n",
    "\n",
    "### üì∏ Example\n",
    "\n",
    "![Example of the landmarks](piano_example.jpg)\n",
    "\n",
    "### üìù Instructions\n",
    "1. Replace the video_path with the path to your actual video file in the script.\n",
    "2. **Click on the landmark points** on a video frame:\n",
    "    - **Blue points (left to right)** first\n",
    "    - **Red points (left to right)** next\n",
    "3. This step is **only required once** for videos from the same YouTube channel, since their viewpoints are usually consistent.\n",
    "4. Replace the video_path with the path to your actual video file in the script.\n",
    "5. The homography matrix will be saved as homography_matrix.npy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: DeprecationWarning: invalid escape sequence '\\/'\n",
      "<>:1: DeprecationWarning: invalid escape sequence '\\/'\n",
      "/var/folders/wd/4h5ts6z93_xb2x7l2x26_3j00000gn/T/ipykernel_23606/1869071400.py:1: DeprecationWarning: invalid escape sequence '\\/'\n",
      "  '''\n",
      "2025-04-03 22:40:56.964 Python[23606:3294427] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-03 22:40:56.964 Python[23606:3294427] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "/var/folders/wd/4h5ts6z93_xb2x7l2x26_3j00000gn/T/ipykernel_23606/1869071400.py:1: DeprecationWarning: invalid escape sequence '\\/'\n",
      "  '''\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m'\u001b[39m, image)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# If the 'q' key is pressed, break from the loop\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# If we have enough points, estimate homography\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Mapping from pixel coordinate (x, y) to world coordinate (x', y') with homography matrix H\n",
    "But real coordinate is different from mujoco coordinate\n",
    "Needs a second mapping from real coordinate (x', y') to mujoco coordinate (x'', y''):\n",
    "x'' = -y'\n",
    "y'' = x'\n",
    "Pixel coordinate:  World coordinate:   Mujoco coordinate:\n",
    "|-------------> x  ^ y'                |--------------> y''\n",
    "|                  |                   |\n",
    "|                  |                   |\n",
    "|                  |                   |\n",
    "|                  |                   |\n",
    "\\/                 |-------------> x'  \\/ \n",
    "y                                      x''          \n",
    "'''\n",
    "# Specify the path to the input video file\n",
    "video_path = 'Stan_1.mp4'  # TODO Replace with your video file's path\n",
    "\n",
    "def click_event(event, x, y, flags, params):\n",
    "    global estimate_homography\n",
    "    \n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        # Record the pixel point\n",
    "        pixel_points.append((x, y))\n",
    "        print(f\"Pixel Point: ({x}, {y})\")\n",
    "        \n",
    "        if len(pixel_points) <= 14:\n",
    "            # Blue\n",
    "            cv2.circle(image, (x, y), 5, (255, 0, 0), -1)\n",
    "        else:\n",
    "            # Red\n",
    "            cv2.circle(image, (x, y), 5, (0, 0, 255), -1)\n",
    "\n",
    "        # Check if we have enough points to estimate homography\n",
    "        if len(pixel_points) == len(real_world_points):\n",
    "            estimate_homography = True\n",
    "            # Save the image\n",
    "            cv2.imwrite('Landmarks.png', image)\n",
    "\n",
    "# Lists to store the correspondence points\n",
    "pixel_points = []\n",
    "real_world_points = []\n",
    "hints = [\"Between key 2 and key 3 top\", \n",
    "            \"Between key 2 and key 3 bottom\",\n",
    "            \"Between key 14 and key 15 top\",\n",
    "            \"Between key 14 and key 15 bottom\",\n",
    "            \"Between key 26 and key 27 top\",\n",
    "            \"Between key 26 and key 27 bottom\",\n",
    "            \"Between key 31 and key 32 top\",\n",
    "            \"Between key 31 and key 32 bottom\",\n",
    "            \"Between key 38 and key 39 top\",\n",
    "            \"Between key 38 and key 39 bottom\",\n",
    "            \"Between key 43 and key 44 top\",\n",
    "            \"Between key 43 and key 44 bottom\",\n",
    "            \"Between key 62 and key 63 top\",\n",
    "            \"Between key 62 and key 63 bottom\",\n",
    "            \"Between key 74 and key 75 top\",\n",
    "            \"Between key 74 and key 75 bottom\",\n",
    "            \"Between key 86 and key 87 top\",\n",
    "            \"Between key 86 and key 87 bottom\",\n",
    "            \"Between key 8 and 10 middle\",\n",
    "            \"Between key 20 and 22 middle\",\n",
    "            \"Between key 32 and 34 middle\",\n",
    "            \"Between key 51 and 53 middle\",\n",
    "            \"Between key 68 and 70 middle\",\n",
    "            \"Between key 80 and 82 middle\"\n",
    "        ]\n",
    "task = piano_with_one_shadow_hand.PianoWithOneShadowHand(\n",
    "    hand_side=HandSide.LEFT,\n",
    "    midi=music.load(\"TwinkleTwinkleRousseau\"),\n",
    "    disable_colorization=True,\n",
    "    change_color_on_activation=True,\n",
    "    trim_silence=True,\n",
    "    control_timestep=0.01,\n",
    "    )\n",
    "\n",
    "env = composer_utils.Environment(\n",
    "    recompile_physics=False, task=task, strip_singleton_obs_buffer_dim=True\n",
    ")\n",
    "white_keys = [2, 14, 26, 43, 62, 74, 86]\n",
    "for key in white_keys:\n",
    "    y = consts.WHITE_KEY_LENGTH/2\n",
    "    x = (env.task.piano._keys[key].pos[1] + env.task.piano._keys[key+1].pos[1])/2\n",
    "    real_world_points.append((x, y))\n",
    "    real_world_points.append((x, -y))\n",
    "\n",
    "black_keys = [9, 21, 33, 52, 69, 81]\n",
    "for key in black_keys:\n",
    "    y = consts.WHITE_KEY_LENGTH/2 - consts.BLACK_KEY_LENGTH\n",
    "    x = (env.task.piano._keys[key-1].pos[1] + env.task.piano._keys[key+1].pos[1])/2\n",
    "    real_world_points.append((x, y))\n",
    "\n",
    "# Flag to indicate when to perform homography estimation\n",
    "estimate_homography = False\n",
    "\n",
    "# Create a VideoCapture object to load the input video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Create a window and set a callback function\n",
    "cv2.namedWindow('Image')\n",
    "cv2.setMouseCallback('Image', click_event)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:   \n",
    "    ret, image = cap.read()\n",
    "\n",
    "while True:\n",
    "    # Display the image\n",
    "    cv2.imshow('Image', image)\n",
    "    \n",
    "    # If the 'q' key is pressed, break from the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "    # If we have enough points, estimate homography\n",
    "    if estimate_homography:\n",
    "        # Convert points to numpy arrays\n",
    "        pts_src = np.array(pixel_points, dtype='float32')\n",
    "        pts_dst = np.array(real_world_points, dtype='float32')\n",
    "        \n",
    "        # Estimate the homography\n",
    "        H, status = cv2.findHomography(pts_src, pts_dst)\n",
    "        print(\"Homography Matrix:\")\n",
    "        print(H)\n",
    "        # Save the homography matrix\n",
    "        np.save('homography_matrix.npy', H)\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Fingering from Video Using Hand Tracking\n",
    "\n",
    "In this step, you'll use hand tracking to generate the fingering information from the video.\n",
    "\n",
    "### üõ†Ô∏è Instructions\n",
    "\n",
    "1. **Replace** `TASK_NAME` in your script with the actual name of your task.\n",
    "2. **Add the corresponding MIDI file** to the folder named exactly the same as your video file.\n",
    "\n",
    "### üéØ Output\n",
    "\n",
    "- Fingering data generated will be saved to a pickle file with the same name as the video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: NoteTrajectory.trim_silence is deprecated. Trim the silence at the MIDI level instead.\n",
      "FPS: 60.0\n",
      "Video width, height: 1280 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "/Users/cheng/Projects/robopianist/venv/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "CTRL_TIMESTEP = 0.05\n",
    "TASK_NAME = \"Stan_1\" # TODO Replace with your task name\n",
    "\n",
    "TASK_MIDI = \"{}.mid\".format(TASK_NAME) \n",
    "\n",
    "midi = music.load(TASK_MIDI)\n",
    "note_traj = midi_file.NoteTrajectory.from_midi(midi, CTRL_TIMESTEP)\n",
    "note_traj = note_traj.trim_silence()\n",
    "start_from = 0 # To align with the video (different for each video)\n",
    "notes = note_traj.notes[start_from:]\n",
    "sustains = note_traj.sustains[start_from:]\n",
    "note_length = len(notes)\n",
    "\n",
    "detector = create_detector()\n",
    "\n",
    "# Specify the path to the input video file\n",
    "video_path = '{}.mp4'.format(TASK_NAME)  # Replace with your video file's path\n",
    "out_filename = 'out.mp4'\n",
    "\n",
    "# Create a VideoCapture object to load the input video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(\"FPS:\", fps)\n",
    "\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "print(\"Video width, height:\", frame_width, frame_height)\n",
    "\n",
    "# Load homography matrix\n",
    "H = np.load('homography_matrix.npy')\n",
    "\n",
    "out = cv2.VideoWriter(out_filename, cv2.VideoWriter_fourcc(\n",
    "    'M', 'J', 'P', 'G'), 10, (frame_width, frame_height))\n",
    "\n",
    "last_timestamp = float('-inf') # Let the first frame be processed -inf\n",
    "timestep = 0\n",
    "last_fingering = []\n",
    "last_keys = []\n",
    "first_timestamp = None\n",
    "# Check if the video was successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    # Get the frame rate of the video\n",
    "    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    timestamp = 0\n",
    "    # Loop through each frame in the video\n",
    "    while timestep < note_length:\n",
    "        # Read the next frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        timestamp = int(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "        if first_timestamp is None:\n",
    "            first_timestamp = timestamp\n",
    "        # Check if the video has ended\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Process the frame here \n",
    "        if timestamp - first_timestamp < CTRL_TIMESTEP * 1000 * timestep:\n",
    "            pass\n",
    "        else:\n",
    "            last_timestamp = timestamp\n",
    "            if timestep >= note_length:\n",
    "                break\n",
    "            keys = [note.key for note in notes[timestep]]\n",
    "            fingering = [note.fingering for note in notes[timestep]]\n",
    "\n",
    "            original_frame = frame.copy()\n",
    "\n",
    "            # brightness_factor = 8 # You can adjust this value to control the brightness\n",
    "            # frame = cv2.addWeighted(frame, brightness_factor, np.zeros_like(frame), 0, 0)\n",
    "\n",
    "            # contrast_factor = 1 # You can adjust this value to control the contrast\n",
    "            # frame = cv2.convertScaleAbs(frame, alpha=contrast_factor, beta=0.5)\n",
    "\n",
    "            # frame = enhance_hand_visibility(frame)\n",
    "            frame_eq, frame = preprocess_frame(frame) # Enhance the local contrast\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            frame.flags.writeable = False\n",
    "            image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "            detection_result = detector.detect_for_video(image, timestamp)\n",
    "            original_frame.flags.writeable = True\n",
    "            \n",
    "            original_frame = draw_landmarks_on_image(original_frame, detection_result)\n",
    "            \n",
    "            original_frame, fingering = extract_finger_based_on_key(original_frame, detection_result, keys, H, \n",
    "                                                                        last_keys=last_keys, last_fingering=last_fingering)\n",
    "            for i, note in enumerate(notes[timestep]):\n",
    "                object.__setattr__(note, 'fingering', fingering[i])\n",
    "            last_fingering = fingering\n",
    "            last_keys = keys\n",
    "\n",
    "            # Write the frame to the output video file\n",
    "            out.write(original_frame)\n",
    "\n",
    "            # Display the frame (you can remove this line if you don't need to display the video)\n",
    "            cv2.imshow('Video Frame', original_frame)\n",
    "            timestep += 1\n",
    "\n",
    "        # Exit the loop if the 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the VideoCapture and close the display window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    # Save the updated notes to pickle file\n",
    "    file_name = '{}.pkl'.format(TASK_NAME)\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(note_traj, f)\n",
    "    os.rename(out_filename, '{}_fingering.mp4'.format(TASK_NAME))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Fingertip Trajectory in MuJoCo Coordinates\n",
    "\n",
    "In this step, you'll convert the detected human fingertip positions into **MuJoCo-compatible coordinates**.\n",
    "\n",
    "### ‚öôÔ∏è How It Works\n",
    "\n",
    "- The system uses **heuristics** to ensure that the generated fingertip trajectory is properly **aligned with the MIDI notes** of the song using the previously estimated fingering.\n",
    "- This alignment helps synchronize finger movements with keypress events.\n",
    "\n",
    "### üíæ Output\n",
    "\n",
    "- The fingertip trajectories are saved as `.npy` files.\n",
    "- Example output filenames:\n",
    "  - `Stan_1_left_hand_action_list.npy`\n",
    "  - `Stan_2_left_hand_action_list.npy`\n",
    "\n",
    "These trajectory files can then be used to drive simulations or robots in the MuJoCo environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 60.0\n",
      "Video width, height: 1280 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "CTRL_TIMESTEP = 0.05\n",
    "\n",
    "TASK_NAME = \"Stan_1\" # TODO Replace with your task name\n",
    "TASK_VIDEO = \"{}.mp4\".format(TASK_NAME)\n",
    "TASK_MIDI = \"{}.mid\".format(TASK_NAME)\n",
    "# Load pickle file\n",
    "with open('{}.pkl'.format(TASK_NAME), 'rb') as f:\n",
    "    note_traj = pickle.load(f)\n",
    "\n",
    "start_from = 0 # To align with the video (different for each video)\n",
    "notes = note_traj.notes[start_from:]\n",
    "sustains = note_traj.sustains[start_from:]\n",
    "note_length = len(notes)\n",
    "\n",
    "detector = create_detector()\n",
    "\n",
    "# Specify the path to the input video file\n",
    "video_path = TASK_VIDEO  # Replace with your video file's path\n",
    "out_filename = 'out.mp4'\n",
    "\n",
    "# Create a VideoCapture object to load the input video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(\"FPS:\", fps)\n",
    "\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "print(\"Video width, height:\", frame_width, frame_height)\n",
    "\n",
    "# Load homography matrix\n",
    "H = np.load('homography_matrix.npy')\n",
    "\n",
    "out = cv2.VideoWriter(out_filename, cv2.VideoWriter_fourcc(\n",
    "    'M', 'J', 'P', 'G'), 10, (frame_width, frame_height))\n",
    "\n",
    "last_timestamp = float('-inf') # Let the first frame be processed -inf\n",
    "timestep = 0\n",
    "left_hand_initial_action_list = HandAction(*np.load('left_hand_initial_action_list.npy'))\n",
    "right_hand_initial_action_list = HandAction(*np.load('right_hand_initial_action_list.npy'))\n",
    "\n",
    "left_hand_action_list = []\n",
    "right_hand_action_list = []\n",
    "last_hand_action_list = [left_hand_initial_action_list, right_hand_initial_action_list]\n",
    "hand_action_list = []\n",
    "first_timestamp = None\n",
    "# Check if the video was successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    # Get the frame rate of the video\n",
    "    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    timestamp = 0\n",
    "    # Loop through each frame in the video\n",
    "    while timestep < note_length:\n",
    "        # Read the next frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        timestamp = int(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "        if first_timestamp is None:\n",
    "            first_timestamp = timestamp\n",
    "        # Check if the video has ended\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Process the frame here \n",
    "        if timestamp - first_timestamp < CTRL_TIMESTEP * 1000 * timestep:\n",
    "            pass\n",
    "        else:\n",
    "            last_timestamp = timestamp\n",
    "            if timestep >= note_length:\n",
    "                break\n",
    "            keys = [note.key for note in notes[timestep]]\n",
    "            fingering = [note.fingering for note in notes[timestep]]\n",
    "\n",
    "            original_frame = frame.copy()\n",
    "\n",
    "            # brightness_factor = 8 # You can adjust this value to control the brightness\n",
    "            # frame = cv2.addWeighted(frame, brightness_factor, np.zeros_like(frame), 0, 0)\n",
    "\n",
    "            # contrast_factor = 1 # You can adjust this value to control the contrast\n",
    "            # frame = cv2.convertScaleAbs(frame, alpha=contrast_factor, beta=0.5)\n",
    "\n",
    "            # frame = enhance_hand_visibility(frame)\n",
    "            frame_eq, frame = preprocess_frame(frame) # Enhance the local contrast\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            frame.flags.writeable = False\n",
    "            image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "            detection_result = detector.detect_for_video(image, timestamp)\n",
    "            original_frame.flags.writeable = True\n",
    "            \n",
    "            original_frame, hand_action_list, handedness_list = \\\n",
    "                process_landmarks(original_frame, detection_result, keys, fingering, H, timestep=timestep)\n",
    "            if len(hand_action_list) == 0:\n",
    "                # No hand detected\n",
    "                hand_action_list = last_hand_action_list\n",
    "            elif len(hand_action_list) == 1:\n",
    "                # Only one hand detected\n",
    "                if handedness_list[0] == 'Left':\n",
    "                    # Use the last right hand action\n",
    "                    hand_action_list = [adjust_hand_action(hand_action_list[0], last_hand_action_list[0]),\n",
    "                                        adjust_hand_action(last_hand_action_list[1])]\n",
    "                else:\n",
    "                    # Use the last left hand action\n",
    "                    hand_action_list = [adjust_hand_action(last_hand_action_list[0]),\n",
    "                                        adjust_hand_action(hand_action_list[0], last_hand_action_list[1])]\n",
    "            else:\n",
    "                # Two hands detected\n",
    "                if last_hand_action_list != []:\n",
    "                    hand_action_list = [adjust_hand_action(hand_action_list[0], last_hand_action_list[0]), \n",
    "                                        adjust_hand_action(hand_action_list[1], last_hand_action_list[1])]\n",
    "                    \n",
    "            last_hand_action_list = hand_action_list\n",
    "            # For testing\n",
    "            val_hand_action(hand_action_list[0])\n",
    "            val_hand_action(hand_action_list[1])\n",
    "            left_hand_action_list.append(hand_action_list[0])\n",
    "            right_hand_action_list.append(hand_action_list[1])\n",
    "            \n",
    "            # Write the frame to the output video file\n",
    "            out.write(original_frame)\n",
    "\n",
    "            # Display the frame (you can remove this line if you don't need to display the video)\n",
    "            cv2.imshow('Video Frame', original_frame)\n",
    "            timestep += 1\n",
    "\n",
    "        # Exit the loop if the 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Write the hand action list to file\n",
    "    left_hand_action_list = np.array(left_hand_action_list)\n",
    "    right_hand_action_list = np.array(right_hand_action_list)\n",
    "    # print(left_hand_action_list.shape)\n",
    "    # print(right_hand_action_list.shape)\n",
    "    np.save('{}_left_hand_action_list.npy'.format(TASK_NAME), left_hand_action_list)\n",
    "    np.save('{}_right_hand_action_list.npy'.format(TASK_NAME), right_hand_action_list)\n",
    "\n",
    "    # Release the VideoCapture and close the display window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "os.rename(out_filename, '{}_mujoco.mp4'.format(TASK_NAME))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Control the Robot Using Inverse Kinematics (IK)\n",
    "\n",
    "In this step, you'll use the previously generated fingertip trajectories to control a robot in a **MuJoCo simulation**.\n",
    "\n",
    "### ü§ñ What Happens Here\n",
    "\n",
    "- The robot is controlled via **Inverse Kinematics (IK)** to **mimic the human hand movements**.\n",
    "- The input to the IK controller is the `.npy` trajectory file created in Step 3.\n",
    "\n",
    "### üéØ Output\n",
    "\n",
    "- The robot's movements are recorded in a `.mp4` file (e.g., `Stan_1_demo.mp4`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: NoteTrajectory.trim_silence is deprecated. Trim the silence at the MIDI level instead.\n",
      "Total steps: 537\n",
      "Total reward: 1748.7674869298935\n",
      "Metrics: {'precision': 0.7895716945996276, 'recall': 0.6932650527622594, 'f1': 0.6899441340782123, 'sustain_precision': 1.0, 'sustain_recall': 1.0, 'sustain_f1': 1.0}\n"
     ]
    }
   ],
   "source": [
    "task_name = \"Stan_1\" # TODO Replace with your task name\n",
    "\n",
    "mjlib = mjbindings.mjlib\n",
    "\n",
    "def play_video(filename: str):\n",
    "    mp4 = open(filename, \"rb\").read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "    return HTML(\n",
    "        \"\"\"\n",
    "  <video controls>\n",
    "        <source src=\"%s\" type=\"video/mp4\">\n",
    "  </video>\n",
    "  \"\"\"\n",
    "        % data_url\n",
    "    )\n",
    "\n",
    "with open('{}.pkl'.format(task_name), 'rb') as f:\n",
    "    note_traj = pickle.load(f)\n",
    "\n",
    "\n",
    "task = piano_with_shadow_hands_res.PianoWithShadowHandsResidual(\n",
    "    # hand_side=HandSide.LEFT,\n",
    "    note_trajectory=note_traj,\n",
    "    # midi=music.load(task_name),\n",
    "    change_color_on_activation=True,\n",
    "    trim_silence=True,\n",
    "    control_timestep=0.05,\n",
    "    disable_hand_collisions=True,\n",
    "    disable_forearm_reward=True,\n",
    "    disable_fingering_reward=False,\n",
    "    midi_start_from=0,\n",
    "    n_steps_lookahead=10,\n",
    "    gravity_compensation=True,\n",
    "    residual_factor=0.03, # 0.03 for after NeverGonnaGiveYouUp\n",
    "    shift=0,\n",
    ")\n",
    "\n",
    "# Load hand action trajectory\n",
    "left_hand_action_list = np.load('{}_left_hand_action_list.npy'.format(task_name))\n",
    "right_hand_action_list = np.load('{}_right_hand_action_list.npy'.format(task_name))\n",
    "\n",
    "# Load trained actions\n",
    "# actions = np.load('trained_songs/{}/actions_{}.npy'.format(task_name, task_name))\n",
    "\n",
    "env = composer_utils.Environment(\n",
    "    recompile_physics=False, task=task, strip_singleton_obs_buffer_dim=True\n",
    ")\n",
    "\n",
    "env = PianoSoundVideoWrapper(\n",
    "    env,\n",
    "    record_every=1,\n",
    "    camera_id=\"piano/top\",\n",
    "    record_dir=\".\",\n",
    ")\n",
    "env = DeepMimicWrapper(env,\n",
    "                      demonstrations_lh=left_hand_action_list,\n",
    "                      demonstrations_rh=right_hand_action_list,\n",
    "                      remove_goal_observation=False,\n",
    "                      mimic_z_axis=False,)\n",
    "env = ResidualWrapper(env, \n",
    "                      demonstrations_lh=left_hand_action_list,\n",
    "                      demonstrations_rh=right_hand_action_list,\n",
    "                      demo_ctrl_timestep=0.05,)\n",
    "env = MidiEvaluationWrapper(\n",
    "    environment=env, deque_size=1\n",
    ")\n",
    "env = CanonicalSpecWrapper(env, clip=True)\n",
    "\n",
    "env = SinglePrecisionWrapper(env)\n",
    "env = DmControlWrapper(env)\n",
    "\n",
    "env = Dm2GymWrapper(env)\n",
    "step = 0\n",
    "err_poses = list()\n",
    "\n",
    "demos = []\n",
    "env = env.env\n",
    "timestep = env.reset()\n",
    "reward = 0\n",
    "# print(env.physics.named.data.ctrl)\n",
    "# raise\n",
    "while not timestep.last():\n",
    "    action = np.zeros(env.action_spec().shape)\n",
    "    timestep = env.step(action)\n",
    "    step += 1\n",
    "    reward += timestep.reward\n",
    "print(f\"Total steps: {step}\")\n",
    "print(f\"Total reward: {reward}\")\n",
    "print(f\"Metrics: {env.get_musical_metrics()}\")\n",
    "\n",
    "play_video(env.latest_filename)\n",
    "\n",
    "# Rename 00001.mp4 as \"./demos/{}.mp4\".format(task_name)\n",
    "os.rename(\"./00001.mp4\",\"{}_demo.mp4\".format(task_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
